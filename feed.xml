<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://ravioli1369.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ravioli1369.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-13T21:08:55+00:00</updated><id>https://ravioli1369.github.io/feed.xml</id><title type="html">blank</title><subtitle>My website. </subtitle><entry><title type="html">faintest of the brightest: grb hunters</title><link href="https://ravioli1369.github.io/blog/2023/grb-hunters/" rel="alternate" type="text/html" title="faintest of the brightest: grb hunters"/><published>2023-07-01T00:00:00+00:00</published><updated>2023-07-01T00:00:00+00:00</updated><id>https://ravioli1369.github.io/blog/2023/grb-hunters</id><content type="html" xml:base="https://ravioli1369.github.io/blog/2023/grb-hunters/"><![CDATA[<p><a href="https://imagine.gsfc.nasa.gov/science/objects/bursts1.html">Gamma-Ray Bursts</a> (GRBs) are fascinating astronomical phenomena that have captured the attention of scientists and researchers around the world. These powerful bursts of gamma-ray radiation, lasting from a fraction of a second to a few minutes, provide valuable insights into the universe’s workings.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/grbalgo-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/grbalgo-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/grbalgo-1400.webp"/> <img src="/assets/img/grbalgo.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <p class="caption-text">The implemented algorithm</p> </div> <p>The GRB Hunters project aims to create a method to detect grbs based on their signal-to-noise ratio in the data from the Cadmium Zinc Telluride Imager (CZTI) onboard ISRO’s AstroSat. By statistically analysing the noise in the data, a quantitative measure of the signal-to-noise ratio was obtained. This was then used to distinguish particle/noise data from real sources of gamma rays.</p> <div class="row mt-3"> <div class="repo p-2 text-center"> <a href="https://github.com/ravioli1369/grbhunters"> <img class="repo-img-light w-100" alt="ravioli1369/grbhunters" src="https://github-readme-stats.vercel.app/api/pin/?username=ravioli1369&amp;repo=grbhunters&amp;theme=catppuccin_latte"/> <img class="repo-img-dark w-100" alt="ravioli1369/grbhunters" src="https://github-readme-stats.vercel.app/api/pin/?username=ravioli1369&amp;repo=grbhunters&amp;theme=catppuccin_mocha"/> </a> </div> <div class="col-sm mt-3 mt-md-0"> The logic was implemented in a python script that can be used to analyse the data from CZTI. On the left is the repository containing all the code developed for the project. </div> </div> <p><br/></p> <div class="post"> <header class="post-header"> Here is the report documenting the theory, logic and implementation of the project: <a href="/assets/pdf/grbhunters.pdf" target="_blank" rel="noopener noreferrer" class="float-right"><i class="fas fa-file-pdf"></i></a> </header><br/> <embed src="/assets/pdf/grbhunters.pdf" width="100%" height="1000px" type="application/pdf"/> </div>]]></content><author><name></name></author><category term="grb"/><category term="astronomy"/><category term="project"/><summary type="html"><![CDATA[a blog post detailing the grb hunters project - a project to detect gamma ray bursts in the data from the czti instrument onboard astrosat. Krittika Summer Projects 2023]]></summary></entry><entry><title type="html">astro image processing</title><link href="https://ravioli1369.github.io/blog/2023/astro-image-processing/" rel="alternate" type="text/html" title="astro image processing"/><published>2023-05-15T00:00:00+00:00</published><updated>2023-05-15T00:00:00+00:00</updated><id>https://ravioli1369.github.io/blog/2023/astro-image-processing</id><content type="html" xml:base="https://ravioli1369.github.io/blog/2023/astro-image-processing/"><![CDATA[<h1 id="using-siril"><strong>Using Siril</strong></h1> <p><a href="https://siril.org/">Siril</a> is an open-source software application for astrophotography, which allows pre-processing and processing of images from any type of camera. This is a very powerful software that allows you to get away with stacking, extracting the background, stretching, color calibration, and much more, all for free, not to mention that it’s available on Windows, MacOS, and Linux. Kinda makes it the perfect app for astro-image processing.</p> <p>To get started, first download Siril using the link provided above and install it. In the realm of amateur astrophotography, we don’t always have the luxury of being able to capture all our calibration frames, be it due to time constraints or just being too tired after a long night of imaging. The default “scripts” Siril provides require that all the calibration frames be present, but there are <a href="https://free-astro.org/index.php?title=Siril:scripts">additional scripts</a> available. Following the instructions provided in the siril:scripts site, we can now begin to process images without all calibration frames, BUT just because we <em>can</em> process without calibration frames <em>does not</em> mean we give up capturing those frames entirely.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/siril_step1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/siril_step1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/siril_step1-1400.webp"/> <img src="/assets/img/siril_step1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> The first step one must do before anything is organizing all your files in folders, namely "lights", "darks", "biases" and "flats". All these folders must lie within the same common folder, additionally, the names of the folders <i>must</i> be the same as mentioned above. Now, we can change our working directory to this common folder by pressing the little home icon at the top left next to the "Open" button and navigating to our folder containing subfolders of our images. </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> Now, click on the "Scripts" tab to reveal a list of scripts. If you have imported the additional scripts mentioned earlier then you should see them in the dropdown as shown on the left. All that's left is to run the correct script according to the data we have. For example, we run the "OSC_preprocessing_WithoutDark" if we are missing dark frames and processing a color RGB image, and we run "Mono_Preprocessing_WithoutFlat" for processing monochrome images without flat frames and so on. <br/> <br/> A thing to note is that once the script is running, it <i>can and will</i> take up large amounts of hard-disk space in a folder it creates called "process". This folder can be deleted once the final image "result.fit" is produced. It is imperative that you don't disturb the process of the script running, additionally, your device may become unresponsive during the process; best to leave it be and return once it's done stacking (should take anywhere between a couple of minutes to half an hour or longer depending on the number of images and processing capabilities of your device). </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/siril_step2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/siril_step2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/siril_step2-1400.webp"/> <img src="/assets/img/siril_step2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/> Following the above steps <em>should</em> (no promises) lead to a “results.fit” file in the same folder containing the folders of all your files. If the script fails for whatever reason, then Siril will provide reasons for the same. Additionally you can follow the guide to <a href="https://siril.org/tutorials/tuto-manual/">manual pre-processing</a> written by Siril.</p> <p>Now we can move on to importing the result.fit file back into Siril by using the “Open” button in the top left corner and navigating to the file. You <em>should</em> (not always the case, it’s possible to see the structure of extremely bright objects like Orion’s Nebula) see a completely pitch-black image with just a few stars poking out of it. Nothing to panic about, this is completely natural and is a direct result of the stacking process.</p> <p>The data in our image is “compressed” into a very small region, which is why we mostly see black. We’ve gotta “stretch” our data in order to see the vast amount of detail that would otherwise be hidden, but before that, some changes to the image must be made.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/siril_step3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/siril_step3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/siril_step3-1400.webp"/> <img src="/assets/img/siril_step3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> Start by first changing the data from a "linear" view to an "autostretch" view. Basically, Siril will stretch the image for us to show a rough idea of the data we are working with so that we can make changes like color calibration, background extraction, etc. This "autostretch" always overexaggerates the image and is quite unrefined. Click on the "Linear" toggle at the bottom and select autostretch and watch the magic happen. You'll go from an almost black image to revealing all the details of the target you've captured. Quite a dramatic but fun thing to watch. This can help show how usable your data actually is before you go through the tedious process of stretching it manually and finding a screw-up in the stacking or capturing phase. </div> </div> <p>Now onto the main changes to be made to the data: cropping, color calibration, and background extraction. With color calibration, the colors of the sky and objects are tweaked to make them more realistic. Background extraction samples the background at many places in the image and looks for a trend in the variations and removes it following a smoothed function to avoid removing nebulae with it.</p> <p>The image should look black and white at this stage and this is because we are seeing the “Red” color channel of the image. In other words, you’re just seeing the data collected by the pixels that gather red light. Similarly, there’s a “Green” and “Blue” channel next to the red channel, all of them being below the “Open” button.</p> <p>Let’s start by cropping the image to remove artifacts near the edges which may be caused by the stacking process. Siril themselves have a great tutorial on these first few steps. For this step, you can follow their tutorial on <a href="https://siril.org/tutorials/tuto-scripts/#first-operation-cropping-the-image">cropping</a>.</p> <p>Now we can perform <a href="https://siril.org/tutorials/tuto-scripts/#removing-background-gradient">background extraction</a> again by following Siril’s guide. Once this is done, we can move on to looking at the “RGB” channel of the image only to find an image that’s completely green, blue, magenta, or any other color that doesn’t represent the sky or the target. This is completely normal and happens due to the white balance setting while taking the image.</p> <p>Once again, we follow the tutorial on Siril’s site and proceed with <a href="https://siril.org/tutorials/tuto-scripts/#colour-calibration-using-photometry">color calibration</a>, however, Siril has used photometric color calibration, a technique that requires you to “plate solve” your image by entering the target you’ve imaged along with telescope and camera details (namely focal length and pixel size). Siril uses this information and detects the stars present in the image and compares the color of those stars to a catalog of known colors and adjusts the colors of the image to best match this catalog. Often this produces great results, but in the case that the plate solve fails, we have to resort to manual color calibration.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> The manual color calibration is just above the photometric color calibration in the dropdown menu. Upon clicking it, we are presented with a popup as shown to the side. For the background reference, select any area on the image that is devoid of many stars, then click on "Use current selection" near the top of the popup. Now click the blue button saying "Background Neutralisation" and that should fix the colors of the image for the most part. Now zoom (Ctrl/Cmd+scroll) into the core of a white star and select another region and follow the same steps. Now click apply.<br/> <br/> Note that you have to go back to any one of the Red, Green, or Blue channels to make selections on the image. </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/siril_step4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/siril_step4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/siril_step4-1400.webp"/> <img src="/assets/img/siril_step4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <iframe width="450" height="250" src="https://www.youtube.com/embed/rFDwGnUwOh8?si=WrhG4AD15xW4ujdW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div class="col-sm mt-3 mt-md-0"> The actual process of stretching is frankly too long to be covered in the form of a guide like this, so I've attached a video that explains a powerful method of stretching that can be done with Siril. Only about half of the video is relevant, just the stretching part, StarNet++ functionality can be ignored for now.<br/> <br/> You can and should perform additional functions like noise reduction, median filter, etc in Siril itself. Play around with the settings to see what works best, there are limitless possibilities. </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> Once you've stretched the image and are happy with how it looks, you can export the image by clicking on the down arrow next to the "Save" button near the top right corner. You can choose the file type it saves just by changing the extension at the end. Common file types are .jpg and .png, although .png is preferred due to its higher quality. </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/siril_step5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/siril_step5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/siril_step5-1400.webp"/> <img src="/assets/img/siril_step5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h2 id="processing-mono-images"><strong>Processing Mono Images</strong></h2> <p>The above guide showed the steps involved in processing a One Shot Color (OSC) image, i.e. an RGB image where the Red, Green, and Blue channels are already part of the image data. However, the typical astrophotography camera has a monochrome (black and white) sensor in order to collect more data in each channel.</p> <p>Filters are used in front of the sensor to isolate various frequencies of light and effectively creates an image for each “channel” of the final image to be produced. 3 separate images are taken, each with a different filter to isolate different frequencies of light and they are mapped to their corresponding colors in software.</p> <p>Common filters that are used are the Sulphur II, Oxygen III, Hydrogen-alpha, and Hydrogen-beta filters that isolate frequencies of light corresponding to the atom’s spectral emissions.</p> <p>The stacking part of mono images remains unchanged from the regular OSC images, the only change being that we have to use the scripts labelled “Mono_”. Once the stacking is complete we <strong>do not</strong> need to perform anything on it and save the files according to the filter being used. Once all 3 or however many images are stacked and the files are correctly named and saved, we can go onto combine the images.</p>]]></content><author><name></name></author><category term="photography"/><category term="astrophotography"/><category term="editing"/><category term="tutorial"/><summary type="html"><![CDATA[learn some basic processing techniques for deep sky images - using Siril.]]></summary></entry><entry><title type="html">the basics</title><link href="https://ravioli1369.github.io/blog/2022/the-basics/" rel="alternate" type="text/html" title="the basics"/><published>2022-10-20T00:00:00+00:00</published><updated>2022-10-20T00:00:00+00:00</updated><id>https://ravioli1369.github.io/blog/2022/the-basics</id><content type="html" xml:base="https://ravioli1369.github.io/blog/2022/the-basics/"><![CDATA[<h1 id="playing-with-light"><strong>Playing with light</strong></h1> <p>There are <strong>three</strong> main ways of controlling the amount and quality of light that a sensor recieves. One being the aperture, another being the shutter speed and lastly we have the ISO. There’s a couple neat little analogies we can use to understand these three qualities.</p> <p>Think about filling an ice cube tray under a tap with running water. In our analogy the running water serves the purpose of light, and the cubes in the tray represent the pixels on a sensor.</p> <h2 id="aperture"><strong>Aperture</strong></h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/aperture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/aperture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/aperture-1400.webp"/> <img src="/assets/img/aperture.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The <strong>aperture</strong> determines exactly how much light is let in through the lens, in the case of our analogy, the aperture would be the diameter of the tap that supplies the water, the larger the diameter, the more the amount of water (light) that can enter the tray (sensor).</p> <p>On a camera, the aperture is usually measured as a ratio between the focal length and the diameter of the opening in the lens, known as the <strong>f-number</strong> or <strong>f-stop</strong>. This approach is used as the size of the opening relative to the focal length is what determines the amount of light that is collected. For example, two lenses of different focal lengths but the same apertures will collect the same amount of light but at very different fields of view.</p> <p>The <strong>focal length</strong> of a lens basically determines how wide of a field of view the camera can see, a lower focal length allows us to capture more of the surroundings in frame, and a higher focal length narrows down our field of view and puts the focus on a single object. Higher focal lengths are used generally for portraits, far away objects like the moon, etc.</p> <p>Say a lens has a focal length of 50mm and the diameter of its opening is 25mm, then its f-number will be 50 divided by 25 which is equal to 2. We denote this by adding an <code class="language-plaintext highlighter-rouge">f</code> in front of the ratio: <code class="language-plaintext highlighter-rouge">f2.0</code> or <code class="language-plaintext highlighter-rouge">f/2.0</code>. The smaller the f-number, the larger is the opening on the lens. It is a bit counterintuitive and there’s no real way of justifying why it is the way it is aside from just convention.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bokeh-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bokeh-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bokeh-1400.webp"/> <img src="/assets/img/bokeh.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Ever wondered how or why some photos have a very blurred background with circular-looking blobs of light? Well, that has to do with something called <strong>depth of field</strong> or <strong>bokeh</strong>, which determines how much of the background is out of focus. Good quality bokeh is a favorite amongst photographers and videographers alike, it helps isolate the subject and allows cinematographers to pull the <em>focus</em> (pun intended) of the audience from one actor to another.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <iframe width="450" height="250" src="https://www.youtube.com/embed/_lZvF-YyP0s?si=tR0uuyyE7p6A9iia&amp;start=310" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div class="col-sm mt-3 mt-md-0"> Here's a nice video that shows how changes in the aperture affect the depth of field produced by the lens. (The video is meant to start at 5:10, watching till 9:12 should be sufficient). </div> </div> <h3 id="how-to-choose-the-correct-aperture"><strong>How to choose the correct aperture?</strong></h3> <p>The choice of aperture comes down to the nature of what a photographer is trying to capture.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cat-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cat-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cat-1400.webp"/> <img src="/assets/img/cat.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/powai_lake-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/powai_lake-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/powai_lake-1400.webp"/> <img src="/assets/img/powai_lake.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> A large aperture (small f-number) paired with a large focal length leads to the maximum amount of bokeh. This kind of setup is perfect for portraits where you try and isolate the subject from the busyness of the background.<br/> <br/> This picture of a cat, shot at f/1.8 and 50mm, shows how the cat is in focus but the lights in the background and the background itself are completely out of focus, helping the viewer to focus on the subject. </div> <div class="col-sm mt-3 mt-md-0"> A smaller aperture (larger f-number) along with a smaller focal length produces very little bokeh and keeps most of the frame in focus, this is perfect for landscapes where every part of a vast terrain needs to be in focus.<br/> <br/> This picture of the Powai Lake, shot at f/11 and 50mm, shows how each part of the lake is in focus, helping the viewer to see the picture as a whole rather than to focus on a specific part. </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/fireworks-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/fireworks-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/fireworks-1400.webp"/> <img src="/assets/img/fireworks.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>There’s another interesting artifact caused by the aperture of a lens which can be seen in the form of spikes on the point sources of light in this picture of a firework. This effect is also known as the <strong>starburst</strong> effect as it is most prominent while taking pictures of the sun. This shot was taken at <code class="language-plaintext highlighter-rouge">f/16</code> which is a very small opening on the lens.</p> <p>If we look at the picture of the lens at the beginning of the aperture subheading, we can see that the lens has seven “blades” forming a heptagon that open and close to change the size of the iris. Now looking again at the above image we can see that there are fourteen spikes in each point source, this specific number comes from the number of blades a lens has. A lens that has an even number of aperture blades produces the same number of spikes, whereas a lens with an odd number of blades produces double the number of spikes, that’s why we see fourteen spikes using a lens that has seven blades.</p> <h2 id="shutter-speed"><strong>Shutter speed</strong></h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/waterdrop-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/waterdrop-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/waterdrop-1400.webp"/> <img src="/assets/img/waterdrop.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Next, we have the <strong>shutter speed</strong> also known as <strong>exposure time</strong> of a camera tells us the duration for which the sensor is exposed to light, if we take the example of our analogy, then the shutter speed would be signified by how long the tap is running or a more accurate way of saying it would be the amount of time the tray is under the tap.</p> <p>The usual way of measuring the shutter speed is in terms of fractions of a second (1/100s, 1/50s, 1/320s and so on). Choosing the right shutter speed is very important when it comes to the sharpness of an image. While photographing people or pets, the preferred shutter speed would be faster than 1/100s (here faster than would mean a numerically smaller value than 1/100) so as to minimise motion blur. With a slower shutter speed (longer exposure) theres more motion blur that can be introduced into an image, the motion blur can be from the subject or from the photographer’s hand itself.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <iframe width="450" height="250" src="https://www.youtube.com/embed/CmjeCchGRQo?si=976aDkaigSkdDyz9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </div> <div class="col-sm mt-3 mt-md-0"> Here's a good video that shows how the shutter of a camera actually works. </div> </div> <h3 id="how-to-choose-the-correct-shutter-speed"><strong>How to choose the correct shutter speed?</strong></h3> <p>The choice of shutter speed again depends on the look and feel the photographer wants their image to have.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/another_waterdrop-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/another_waterdrop-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/another_waterdrop-1400.webp"/> <img src="/assets/img/another_waterdrop.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/welding-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/welding-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/welding-1400.webp"/> <img src="/assets/img/welding.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> Sometimes a photographer would want to capture a very sharp image with every little detail frozen in time. For such an effect a faster shutter speed is chosen, usually something faster than 1/1000s, but depends entirely on how fast the subject is.<br/> <br/> This picture of a waterdrop that fell in a tub of water for instance, was shot with an exposure of 1/5000s. Every part of the drop is tack sharp and no motion blur can be seen. </div> <div class="col-sm mt-3 mt-md-0"> Othertimes they would want an intentional blur to signify the movement of an object. It's an interesting way of showing that an object is moving in a photograph rather than a video. Anything from 1/10s to multiple seconds would work.<br/> <br/> Here is a picture of a man welding, shot with an exposure of 5s shows trails of hot molten metal falling off of the weld. We can also see the movement of the arm on the right side of the image. </div> </div> <h2 id="iso"><strong>ISO</strong></h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/sanat-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/sanat-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/sanat-1400.webp"/> <img src="/assets/img/sanat.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Finally we come to the <strong>ISO</strong> of a sensor. ISO stands for International Organisation of Standards, which basically means nothing without a backstory. Back in the days of film photography, the sensitivity of the film (the amount of silver salts present in the sheet of film) would act as a third way of increasing or decreasing the amount of “light” in a photo. The term light is used handwavily here as no extra light is falling on the film itself, rather a higher ISO film is just more sensitive to the already present light, in other words, this is “artificial light”.</p> <p>With higher ISOs, there comes a trade off in the image quality. A higher ISO requires more silver salts in the film roll which creates a “grain” that can be seen in the final result. This grain is seen as small dots (kind of like pepper flakes) all over the image. We can see such a result in the above image, especially along the right side.</p> <p>In digital photography, the ISO represents the sensitivity of the sensor itself to light. This sensitivity is changed electronically through amplifiers, i.e. the voltage itself is amplified to give the perception of more light. All this amplification leads to what we call “noise” for a digital camera. This noise is basically pixels that are either too sensitive to the light or not sensitive enough, and is mainly caused because of very minute imperfections in the manufacturing process.</p> <h3 id="how-to-choose-the-correct-iso"><strong>How to choose the correct ISO?</strong></h3> <p>When it comes to ISO, there isnt really an artistic reason to choose ISOs like there was with shutter speed or aperture. Once a photographer is satisfied with their choice of shutter speed and aperture, they then move onto the ISO and adjust it till they are happy with their exposure. Usually we try and stay at the lowest possible ISO that gives a perfectly exposed image so as to reduce noise. However, grain can be added back into the photo during post processing (editing) to achieve a filmic look which is quite a popular look and feel nowadays.</p>]]></content><author><name></name></author><category term="photography"/><category term="tutorial"/><summary type="html"><![CDATA[learn the absolute basics of photography - light and its interactions]]></summary></entry></feed>